{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-predict.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPSlFmcvqpP87fEGiUoXTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rangsimanketkaew/learning-to-smell/blob/main/train-predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4axEOC-h0_D"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MA3c_LX19wf"
      },
      "source": [
        "import os\r\n",
        "data_dir = \"https://raw.githubusercontent.com/rangsimanketkaew/learning-to-smell/main/data/\"\r\n",
        "files = ['train.csv', 'test.csv', 'vocabulary.txt']\r\n",
        "for i in files:\r\n",
        "    if not os.path.isfile(i):\r\n",
        "        os.system(\"wget \" + data_dir + i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKOoAI3Ah4mr"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6IZJvqyaxDM"
      },
      "source": [
        "%%bash\r\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\r\n",
        "MINICONDA_PREFIX=/usr/local\r\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\r\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\r\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\r\n",
        "\r\n",
        "conda install --channel defaults conda python=3.7 --yes\r\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lGheTLKopeP"
      },
      "source": [
        "%%bash\r\n",
        "conda install -q -y --prefix /usr/local/ -c conda-forge scikit-learn tensorflow-gpu rdkit\r\n",
        "conda --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQw8MD1tdG83"
      },
      "source": [
        "%%bash\r\n",
        "# !source /usr/local/etc/profile.d/conda.sh\r\n",
        "python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAr7wxYUiOvV"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exBr7OZhay25"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sys\r\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages')\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import rdkit\r\n",
        "from rdkit import Chem, DataStructs\r\n",
        "from rdkit.Chem import AllChem, rdMolDescriptors, Descriptors\r\n",
        "from rdkit.Chem.EState import Fingerprinter\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\r\n",
        "from sklearn.model_selection import cross_validate, train_test_split\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "# tf.compat.v1.enable_eager_execution()  # usually turn on by default\r\n",
        "# from tensorflow.python.framework.ops import disable_eager_execution\r\n",
        "# disable_eager_execution()\r\n",
        "import tensorflow_addons as tfa\r\n",
        "from tensorflow.keras.models import Sequential, save_model\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout, BatchNormalization, ReLU, LeakyReLU\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\r\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, Adamax, SGD\r\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\r\n",
        "from tensorflow.keras.constraints import max_norm\r\n",
        "from tensorflow.keras.utils import multi_gpu_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXVIpR9lByT"
      },
      "source": [
        "## Loss implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzcZwgDPi-sY"
      },
      "source": [
        "import tensorflow_addons as tfa\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "\r\n",
        "def humming_loss(y_true, y_pred):\r\n",
        "    # tf.config.run_functions_eagerly(True)\r\n",
        "    \"\"\"Hamming Loss\"\"\"\r\n",
        "    return tfa.metrics.hamming.hamming_loss_fn(y_true=y_true, y_pred=y_pred, mode=\"multiclass\", threshold=0.8)\r\n",
        "\r\n",
        "def npair_loss(y_true, y_pred):\r\n",
        "    \"\"\"NPair Loss\"\"\"\r\n",
        "    return tfa.losses.npairs_multilabel_loss(y_true=y_true, y_pred=y_pred)\r\n",
        "\r\n",
        "def jaccard_loss(y_true, y_pred, smooth=100):\r\n",
        "    \"\"\"Jaccard distance for semantic segmentation.\r\n",
        "    Also known as the intersection-over-union loss.\r\n",
        "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\r\n",
        "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\r\n",
        "    Reference:  \r\n",
        "        - [What is a good evaluation measure for semantic segmentation?](\r\n",
        "           http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf)\r\n",
        "    \"\"\"\r\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\r\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\r\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\r\n",
        "    return (1 - jac) * smooth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhxfNuUIlNPG"
      },
      "source": [
        "## Metric implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jObhGNYxlQP-"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "# from tensorflow.python.framework.ops import disable_eager_execution\r\n",
        "# disable_eager_execution()\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "\r\n",
        "\r\n",
        "def jaccard_tensorflow(y_true, y_pred):\r\n",
        "    \"\"\"Jaccard score of Tensor in tensorflow for graph mode.\r\n",
        "    \"\"\"\r\n",
        "    intersection = tf.sets.intersection(y_true[None:], y_pred[None:])\r\n",
        "    intersection = tf.sparse.to_dense(intersection)[0]\r\n",
        "    union = tf.sets.union(y_true[None:], y_pred[None:])\r\n",
        "    union = tf.sparse.to_dense(union)[0]\r\n",
        "    return float(len(intersection) / len(union))\r\n",
        "\r\n",
        "def jaccard_tensorflow_eager(y_true, y_pred):\r\n",
        "    \"\"\"Jaccard score with built-in function in tensorflow in eager mode.\r\n",
        "    \"\"\"\r\n",
        "    set1 = set(y_true.numpy())\r\n",
        "    set2 = set(y_pred.numpy())\r\n",
        "    return float((len(set1.intersection(set2))) / (len(set1.union(set2))))\r\n",
        "\r\n",
        "def jaccard_from_keras_cont(y_true, y_pred):\r\n",
        "    \"\"\"Jaccard score for keras.\r\n",
        "    Taken directly from https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/jaccard.py\r\n",
        "    \"\"\"\r\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\r\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\r\n",
        "    jac = (intersection) / (sum_ - intersection)\r\n",
        "    return (1 - jac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gubih0cWhtet"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpy7E3w-W49C"
      },
      "source": [
        "def smiles_encoder(smiles, maxlen=240):\r\n",
        "    \"\"\"Calculate simple encoder from SMILES\r\n",
        "    Example:\r\n",
        "    >>> train_set_enc = np.array([smiles_encoder(i) for i in train_set])\r\n",
        "    \"\"\"\r\n",
        "    SMILES_CHARS = [\r\n",
        "    '#', '%', '(', ')', '+', '-', '.', '/',\r\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\r\n",
        "    '=', '@',\r\n",
        "    'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P',\r\n",
        "    'R', 'S', 'T', 'V', 'X', 'Z',\r\n",
        "    '[', '\\\\', ']',\r\n",
        "    'a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's',\r\n",
        "    't', 'u']\r\n",
        "\r\n",
        "    smi2index = dict((c, i) for i, c in enumerate(SMILES_CHARS))\r\n",
        "    index2smi = dict((i, c) for i, c in enumerate(SMILES_CHARS))\r\n",
        "    smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles))\r\n",
        "    X = np.zeros((maxlen, len(SMILES_CHARS)))\r\n",
        "    for i, c in enumerate(smiles):\r\n",
        "        X[i, smi2index[c]] = 1\r\n",
        "    return X\r\n",
        "\r\n",
        "def smiles_decoder(X):\r\n",
        "    smi = ''\r\n",
        "    X = X.argmax(axis=-1)\r\n",
        "    for i in X:\r\n",
        "        smi += index2smi[i]\r\n",
        "    return smi\r\n",
        "\r\n",
        "def maccs_fp(smiles):\r\n",
        "    mol = Chem.MolFromSmiles(smiles)\r\n",
        "    fp = MACCSkeys.GenMACCSKeys(mol)\r\n",
        "    npfp = np.array(list(fp.ToBitString())).astype('int8')\r\n",
        "    return npfp\r\n",
        "\r\n",
        "def morgan_fp(smiles):\r\n",
        "    mol = Chem.MolFromSmiles(smiles)\r\n",
        "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)\r\n",
        "    npfp = np.array(list(fp.ToBitString())).astype('int8')\r\n",
        "    return npfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YcaG9pIXGw4"
      },
      "source": [
        "train_set = pd.read_csv(\"train.csv\")\r\n",
        "# train_set = pd.read_csv(\"data/augmentation/train_aug_random20.csv\")\r\n",
        "train_set, train_label = list(train_set['SMILES']), list(train_set['SENTENCE'])\r\n",
        "train_label = [i.split(',') for i in train_label]\r\n",
        "\r\n",
        "test_set = pd.read_csv(\"test.csv\")\r\n",
        "test_set = list(test_set['SMILES'])\r\n",
        "\r\n",
        "vocab = open(\"vocabulary.txt\", 'r').read().split(\"\\n\")\r\n",
        "\r\n",
        "# count the number of occurences for each label\r\n",
        "# train_label_sub = [item for sublist in train_label for item in sublist]\r\n",
        "# counts = dict((x, train_label_sub.count(x)) for x in set(train_label_sub))\r\n",
        "# pprint(counts)\r\n",
        "\r\n",
        "# Morgan encoding\r\n",
        "print(f\"Size of train set: {len(train_set)}\")\r\n",
        "fingerprint = np.array([morgan_fp(i) for i in train_set])\r\n",
        "np.savez_compressed(\"train_set_fingerprint_1024bits_radius2.npz\", morgan=fingerprint)\r\n",
        "print(\"NumPy compressed file has been saved!\")\r\n",
        "\r\n",
        "fingerprint = np.array([morgan_fp(i) for i in test_set])\r\n",
        "np.savez_compressed(\"test_set_fingerprint_1024bits_radius2.npz\", morgan=fingerprint)\r\n",
        "print(\"NumPy compressed file has been saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Men2AhvjRLl"
      },
      "source": [
        "## Parameter configuration\r\n",
        "User-define parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2P9l0GsjF9-"
      },
      "source": [
        "# Hyper parameter\r\n",
        "N_EPOCHS = 200\r\n",
        "BATCH_SIZE = 32\r\n",
        "ACT_HIDDEN = LeakyReLU(alpha=0.2)\r\n",
        "ACT_OUTPUT = 'sigmoid'\r\n",
        "DROPOUT = 0.2\r\n",
        "KERNEL_REG = l1_l2(l1=1e-5, l2=1e-4)\r\n",
        "BIAS_REG = l2(1e-4)\r\n",
        "ACTI_REG = l2(1e-5)\r\n",
        "TRAIN_WITH_VALID = True\r\n",
        "VALID_SPLIT = 0.9\r\n",
        "# GPU = 2\r\n",
        "\r\n",
        "# OPTIMIZER = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\r\n",
        "OPTIMIZER = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\n",
        "# OPTIMIZER = Adamax(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\n",
        "# OPTIMIZER = Adagrad(learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07, name=\"Adagrad\")\r\n",
        "\r\n",
        "# LOSS = \"binary_crossentropy\"\r\n",
        "LOSS = \"categorical_crossentropy\"\r\n",
        "# LOSS = \"KLDivergence\"\r\n",
        "# LOSS = loss.jaccard_loss\r\n",
        "# LOSS = tf.nn.sigmoid_cross_entropy_with_logits\r\n",
        "\r\n",
        "METRICS = ['accuracy']\r\n",
        "# METRICS = [metric.jaccard_5sentences]\r\n",
        "NAME_CHECKPOINT = 'model_checkpoint.h5'\r\n",
        "PATH_SAVE_MODEL = 'model.h5'\r\n",
        "# SAVE_PREDICTION = True\r\n",
        "# SHOW_FIGURE = True\r\n",
        "\r\n",
        "if os.name == \"posix\": os.system(\"export HDF5_USE_FILE_LOCKING=FALSE\")\r\n",
        "\r\n",
        "# Callback\r\n",
        "checkpointer = ModelCheckpoint(filepath=NAME_CHECKPOINT, monitor='val_acc', mode='max',\r\n",
        "                               verbose=0, save_best_only=False, save_weights_only=False)\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=0.00001, verbose=1)\r\n",
        "earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=30, mode='auto', verbose=1)\r\n",
        "# monitor='val_loss' can also be used if train with validation\r\n",
        "\r\n",
        "cb = [checkpointer, reduce_lr, earlystop]\r\n",
        "\r\n",
        "# plot\r\n",
        "HIST_ACC, HIST_VAL_ACC = 'accuracy', 'val_accuracy'\r\n",
        "HIST_LOSS, HIST_VAL_LOSS = 'loss', 'val_loss'\r\n",
        "\r\n",
        "submission_file_path = \"submission.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhFrqb7lc8_"
      },
      "source": [
        "## GPU configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llXbYbW3jDfZ"
      },
      "source": [
        "# Fix GPU memory growth\r\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
        "if gpus:\r\n",
        "    try:\r\n",
        "        # Currently, memory growth needs to be the same across GPUs\r\n",
        "        for gpu in gpus:\r\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\r\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n",
        "    except RuntimeError as e:\r\n",
        "        # Memory growth must be set before GPUs have been initialized\r\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7eRXTPglolh"
      },
      "source": [
        "## Prepare training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_wfxOIxjlBe"
      },
      "source": [
        "## Training data\r\n",
        "train_set_enc = np.load(\"train_set_all_descriptors.npz\")[\"features\"]\r\n",
        "train_set_fp = np.load(\"train_set_fingerprint_1024bits_radius2.npz\")[\"morgan\"]\r\n",
        "train_set_enc = np.concatenate((train_set_enc, train_set_fp), axis=1)\r\n",
        "\r\n",
        "## Test data\r\n",
        "test_set_enc = np.load(\"test_set_all_descriptors.npz\")[\"features\"]\r\n",
        "test_set_fp = np.load(\"test_set_fingerprint_1024bits_radius2.npz\")[\"morgan\"]\r\n",
        "test_set_enc = np.concatenate((test_set_enc, test_set_fp), axis=1)\r\n",
        "\r\n",
        "## Train label\r\n",
        "train_label = list(pd.read_csv(\"train.csv\")['SENTENCE'])\r\n",
        "train_label = [i.split(',') for i in train_label]\r\n",
        "vocab = open(\"vocabulary.txt\", 'r').read().split(\"\\n\")\r\n",
        "train_label_enc = np.zeros((len(train_label), len(vocab)), dtype=np.float32)\r\n",
        "for i in range(len(train_label)):\r\n",
        "    train_label_enc[i] = onehot_sentence(vocab, train_label[i])\r\n",
        "\r\n",
        "print(train_set_enc.shape)\r\n",
        "print(test_set_enc.shape)\r\n",
        "print(train_label_enc.shape)\r\n",
        "assert train_set_enc.shape[0] == train_label_enc.shape[0]\r\n",
        "train_set, train_label = train_set_enc, train_label_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNp6Pjzfjq-o"
      },
      "source": [
        "## Shuffle dataset before splitting\r\n",
        "# index = np.arange(train_set_enc.shape[0])\r\n",
        "# np.random.shuffle(index)\r\n",
        "# train_set_enc, train_label_enc = train_set_enc[index], train_label_enc[index]\r\n",
        "\r\n",
        "# # Split train set --> real train set + validation set\r\n",
        "# train_set, valid_set, train_label, valid_label = train_test_split(train_set_enc, train_label_enc, test_size=1-VALID_SPLIT, random_state=0)\r\n",
        "# validation_train_test = (valid_set, valid_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ceI-VTjtFB"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf3BDNSFjztW"
      },
      "source": [
        "model = Sequential([\r\n",
        "    Flatten(input_shape=(test_set_enc.shape[1],)),\r\n",
        "    # Dropout(0.2, input_shape=(8192,)),\r\n",
        "    Dense(128, activation=ACT_HIDDEN, kernel_regularizer=KERNEL_REG, bias_regularizer=BIAS_REG, activity_regularizer=ACTI_REG),\r\n",
        "    Dropout(DROPOUT),\r\n",
        "    BatchNormalization(),\r\n",
        "    Dense(128, activation=ACT_HIDDEN, kernel_regularizer=KERNEL_REG, bias_regularizer=BIAS_REG, activity_regularizer=ACTI_REG),\r\n",
        "    Dropout(DROPOUT),\r\n",
        "    BatchNormalization(), \r\n",
        "    Dense(128, activation=ACT_HIDDEN, kernel_regularizer=KERNEL_REG, bias_regularizer=BIAS_REG, activity_regularizer=ACTI_REG),\r\n",
        "    Dense(109, activation=ACT_OUTPUT),\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nxzcdwtj5hn"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgBFpI05j4wm"
      },
      "source": [
        "# Now can train only on a single GPU - failed with multi-GPUs. Needs to be fixed!\r\n",
        "history = model.fit(\r\n",
        "    train_set,\r\n",
        "    train_label,\r\n",
        "    validation_split=VALID_SPLIT,\r\n",
        "    # validation_data=(valid_set, valid_label),\r\n",
        "    shuffle=False,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    epochs=N_EPOCHS,\r\n",
        "    use_multiprocessing=True,\r\n",
        "    verbose=1,\r\n",
        "    callbacks=[cb]\r\n",
        ")\r\n",
        "\r\n",
        "# Save model\r\n",
        "save_model(model, PATH_SAVE_MODEL, overwrite=True, include_optimizer=True, save_format='h5', signatures=None, options=None)\r\n",
        "\r\n",
        "print(f\"Model has been saved to {PATH_SAVE_MODEL}\")\r\n",
        "print(f\"Checkpoint has been saved to {NAME_CHECKPOINT}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvIQsJHnlz1X"
      },
      "source": [
        "## Training output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-QdZRNl2m5"
      },
      "source": [
        "# list all data in history\r\n",
        "# pprint(history.history.keys())\r\n",
        "print(f\"\\nLast Accuracy     : {history.history[HIST_ACC][-1]}\")\r\n",
        "print(f\"Max  Accuracy     : {np.max(history.history[HIST_ACC])}\")\r\n",
        "if TRAIN_WITH_VALID: print(f\"Last Val accuracy : {history.history[HIST_VAL_ACC][-1]}\")\r\n",
        "if TRAIN_WITH_VALID: print(f\"Max  Val accuracy : {np.max(history.history[HIST_VAL_ACC])}\")\r\n",
        "print(\"-----------\")\r\n",
        "print(f\"Min loss         : {np.min(history.history[HIST_LOSS])}\")\r\n",
        "if TRAIN_WITH_VALID: print(f\"Min val loss     : {np.min(history.history[HIST_VAL_LOSS])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCIh04qimDsu"
      },
      "source": [
        "# summarize history for accuracy\r\n",
        "plt.figure(1)\r\n",
        "plt.plot(history.history[HIST_ACC])\r\n",
        "if TRAIN_WITH_VALID: plt.plot(history.history[HIST_VAL_ACC])\r\n",
        "plt.title('model accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'valid'], loc='upper left')\r\n",
        "\r\n",
        "# summarize history for loss\r\n",
        "plt.figure(2)\r\n",
        "plt.plot(history.history[HIST_LOSS])\r\n",
        "if TRAIN_WITH_VALID: plt.plot(history.history[HIST_VAL_LOSS])\r\n",
        "plt.title('model loss')\r\n",
        "plt.ylabel('loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'valid'], loc='upper left')\r\n",
        "plt.show()  # show two plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3nK5ymKmDF4"
      },
      "source": [
        "## Evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ_1pAkImJhw"
      },
      "source": [
        "# test_loss, test_acc = model.evaluate(valid_set_enc,  valid_label_enc, verbose=1)\r\n",
        "# print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWpUPlB0mNHW"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmQ8h2G8mKma"
      },
      "source": [
        "pred = model.predict(test_set_enc)\r\n",
        "\r\n",
        "# Choose the top 15 predictions for each sample and group by 3\r\n",
        "ind2word = {i: x for i, x in enumerate(vocab)}\r\n",
        "pred_for_sub = []\r\n",
        "for i in range(pred.shape[0]):\r\n",
        "    labels = [ind2word[i] for i in list(pred[i, :].argsort()[-15:][::-1])]\r\n",
        "    labels_seq = []\r\n",
        "    # for i in range(0, 15, 3):\r\n",
        "    #     labels_seq.append(\",\".join(labels[i:(i+3)]))\r\n",
        "    labels_seq.append(\",\".join([labels[0], labels[1], labels[2]]))\r\n",
        "    labels_seq.append(\",\".join([labels[0], labels[1], labels[3]]))\r\n",
        "    labels_seq.append(\",\".join([labels[0], labels[1], labels[4]]))\r\n",
        "    labels_seq.append(\",\".join([labels[0], labels[1], labels[5]]))\r\n",
        "    labels_seq.append(\",\".join([labels[0], labels[1], labels[6]]))\r\n",
        "    pred_for_sub.append(\";\".join(labels_seq))\r\n",
        "\r\n",
        "test_set = pd.read_csv(\"data/test.csv\")\r\n",
        "test_set = list(test_set['SMILES'])\r\n",
        "pred_label = {\r\n",
        "    'SMILES': test_set,\r\n",
        "    'PREDICTIONS': pred_for_sub\r\n",
        "}\r\n",
        "df = pd.DataFrame(pred_label)\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBesIOAEmdxs"
      },
      "source": [
        "# Save prediction as csv\r\n",
        "print(f\"Writing Submission (csv) to : {submission_file_path}\")\r\n",
        "df.to_csv(\r\n",
        "    submission_file_path,\r\n",
        "    index=False\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gGbFxgeiO41"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}