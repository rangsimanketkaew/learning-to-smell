{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepchem'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a94f48cd7607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdeepchem\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepchem'"
     ]
    }
   ],
   "source": [
    "## Learning to smell\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import deepchem as dc\n",
    "import rdkit\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graph(data): #The function is to preprocessed the adjacency matrix, returning the normalized adjacency matrix in the form of numpy array for feeding into the model\n",
    "    adj_ = data + sp.eye(data.shape[0]) \n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = np.diag(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt)\n",
    "    return np.array(adj_normalized)\n",
    "\n",
    "def smiles_get_features(a): #This function will return the smiles code into list of feature for each atoms\n",
    "    m = rdkit.Chem.MolFromSmiles(a)\n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    features = featurizer.featurize([m])[0]\n",
    "    if isinstance(features, np.ndarray):\n",
    "        return pd.np.nan\n",
    "    atom_features = features.get_atom_features() # initial atom feature vectors\n",
    "    if atom_features.shape[0] > 60:\n",
    "        return pd.np.nan\n",
    "    return atom_features\n",
    "\n",
    "def smiles_get_adj(a): #This function retrieve the adjacency matrix from the molecule\n",
    "    m = rdkit.Chem.MolFromSmiles(a)\n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    features = featurizer.featurize([m])[0]\n",
    "    if isinstance(features, np.ndarray):\n",
    "        return pd.np.nan\n",
    "    adj_list = features.get_adjacency_list() # adjacency list (neighbor list)\n",
    "    adj=np.zeros((len(adj_list), len(adj_list))) # convert adjacency list into adjacency matrix \"A\"\n",
    "    if len(adj_list) > 60:\n",
    "        return pd.np.nan\n",
    "    return adj_list\n",
    "\n",
    "def sim_graph(smile):\n",
    "    mol = rdkit.Chem.MolFromSmiles(smile)\n",
    "    if mol is None:\n",
    "        return pd.np.nan\n",
    "    Chem.Kekulize(mol)\n",
    "    atoms = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    am = Chem.GetAdjacencyMatrix(mol,useBO=True)\n",
    "    if len(atoms)>60:\n",
    "        return pd.np.nan\n",
    "    for i,atom in enumerate(atoms):\n",
    "        am[i,i] = atom\n",
    "    return am\n",
    "\n",
    "def get_max_dim(d): #This funcion is used to find the maximum dimension the set of data contain\n",
    "    maxdim = 0\n",
    "    for i in d:\n",
    "        if i.shape[0]>maxdim:\n",
    "            maxdim = i.shape[0]\n",
    "    return maxdim\n",
    "\n",
    "def pad_up_to(t, max_in_dims, constant_values=0): #This function is used to pad the data up to a given dimension\n",
    "    s = t.shape\n",
    "    size = np.subtract(max_in_dims, s)\n",
    "    return np.pad(t, ((0,size[0]),(0,size[1])), 'constant', constant_values=constant_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "\n",
    "train_set = pd.read_csv(\"data/train.csv\")\n",
    "test_set = pd.read_csv(\"data/test.csv\")\n",
    "sample_sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "vocab = open(\"data/vocabulary.txt\", 'r').read().split(\"\\n\")\n",
    "\n",
    "print(f\"size of training {train_set.shape}\")\n",
    "print(f\"size of testing {test_set.shape}\")\n",
    "# print(type(vocab))\n",
    "\n",
    "# Prepare training set\n",
    "# Generate image\n",
    "\n",
    "# Prepare test set\n",
    "def onehot_sentence(sentence):\n",
    "    l = np.zeros(len(vocab))\n",
    "    for label in sentence.split(','):\n",
    "        l[vocab.index(label)] = 1\n",
    "    return l\n",
    "\n",
    "test_vocab = np.zeros((train_set.shape[0], len(vocab)), dtype=np.float32)\n",
    "\n",
    "for i in range(train_set.shape[0]):\n",
    "    test_vocab[i] = onehot_sentence(train_set.SENTENCE.iloc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
